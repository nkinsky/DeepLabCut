{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0gDJMy1ywm8"
   },
   "source": [
    "# DeepLabCut Toolbox - Open-Field DEMO\n",
    "https://github.com/AlexEMG/DeepLabCut\n",
    "\n",
    "#### The notebook accompanies the following user-guide:\n",
    "\n",
    "Nath\\*, Mathis\\* et al. *Using DeepLabCut for markerless pose estimation during behavior across species* Nature Protocols, 2019: https://www.nature.com/articles/s41596-019-0176-0\n",
    "\n",
    "This notebook illustrates how to:\n",
    "- load the demo project\n",
    "- train a network\n",
    "- evaluate a network\n",
    "- analyze a novel video\n",
    "- create an automatically labeled video \n",
    "- plot the trajectories \n",
    "- identify outlier frames\n",
    "- annotate the outlier frames manually\n",
    "- merge the data sets and update the training set\n",
    "- train a network\n",
    "\n",
    "Note: This notebook starts from an already initialized project with labeled data.\n",
    "\n",
    "\n",
    "The data is a subset from *DeepLabCut: markerless pose estimation of user-defined body parts with deep learning* https://www.nature.com/articles/s41593-018-0209-y (this subset was not used to train models that are shown or evaluated in our paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTtJxcQ7ywnB"
   },
   "outputs": [],
   "source": [
    "# Importing the toolbox (takes several seconds)\n",
    "import deeplabcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOEHc0MeywnJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded, now creating training data...\n",
      "C:\\Users\\Public\\GitHub\\DeepLabCut\\examples\\openfield-Pranav-2018-10-30\\training-datasets\\iteration-0\\UnaugmentedDataSet_openfieldOct30  already exists!\n",
      "C:\\Users\\Public\\GitHub\\DeepLabCut\\examples\\openfield-Pranav-2018-10-30\\dlc-models\\iteration-0\\openfieldOct30-trainset95shuffle1  already exists!\n",
      "C:\\Users\\Public\\GitHub\\DeepLabCut\\examples\\openfield-Pranav-2018-10-30\\dlc-models\\iteration-0\\openfieldOct30-trainset95shuffle1/train  already exists!\n",
      "C:\\Users\\Public\\GitHub\\DeepLabCut\\examples\\openfield-Pranav-2018-10-30\\dlc-models\\iteration-0\\openfieldOct30-trainset95shuffle1/test  already exists!\n",
      "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
     ]
    }
   ],
   "source": [
    "# Loading example data set:\n",
    "import os\n",
    "# Note that parameters of this project can be seen at: *openfield-Pranav-2018-10-30/config.yaml*\n",
    "from pathlib import Path\n",
    "path_config_file = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30/config.yaml')\n",
    "deeplabcut.load_demo_data(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROlflqQLywnP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating images with labels by Pranav.\n",
      "/home/mackenzie/DEEPLABCUT/3D/DeepLabCut2.0-master/examples/openfield-Pranav-2018-10-30/labeled-data/m4s1_labeled  already exists!\n",
      "They are stored in the following folder: /home/mackenzie/DEEPLABCUT/3D/DeepLabCut2.0-master/examples/openfield-Pranav-2018-10-30/labeled-data/m4s1_labeled.\n",
      "Attention: /home/mackenzie/DEEPLABCUT/3D/DeepLabCut2.0-master/examples/openfield-Pranav-2018-10-30/labeled-data/short_mp3 does not appear to have labeled data!\n",
      "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!\n"
     ]
    }
   ],
   "source": [
    "#[OPTIONAL] Perhaps plot the labels to see how the frames were annotated:\n",
    "#(note, this project was created in Linux, so you might have an error in Windows, but this is an optional step)\n",
    "deeplabcut.check_labels(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9H7eqDLywnV"
   },
   "source": [
    "## Start training of Feature Detectors\n",
    "This function trains the network for a specific shuffle of the training dataset. The user can set various parameters in */openfield-Pranav-2018-10-30/dlc-models/.../pose_cfg.yaml*. \n",
    "\n",
    "Training can be stopped at any time. Note that the weights are only stored every 'save_iters' steps. For this demo the state it is advisable to store & display the progress very often. In practice this is inefficient.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jg96O2acywnW",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'all_joints': [[0], [1], [2], [3]],\n",
      " 'all_joints_names': ['snout', 'leftear', 'rightear', 'tailbase'],\n",
      " 'alpha_r': 0.02,\n",
      " 'batch_size': 1,\n",
      " 'crop_pad': 0,\n",
      " 'cropratio': 0.4,\n",
      " 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_openfieldOct30\\\\openfield_Pranav95shuffle1.mat',\n",
      " 'dataset_type': 'imgaug',\n",
      " 'decay_steps': 30000,\n",
      " 'deterministic': False,\n",
      " 'display_iters': 1000,\n",
      " 'fg_fraction': 0.25,\n",
      " 'global_scale': 0.8,\n",
      " 'init_weights': 'C:\\\\Users\\\\nkinsky\\\\.conda\\\\envs\\\\DLC-GPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt',\n",
      " 'intermediate_supervision': False,\n",
      " 'intermediate_supervision_layer': 12,\n",
      " 'location_refinement': True,\n",
      " 'locref_huber_loss': True,\n",
      " 'locref_loss_weight': 0.05,\n",
      " 'locref_stdev': 7.2801,\n",
      " 'log_dir': 'log',\n",
      " 'lr_init': 0.0005,\n",
      " 'max_input_size': 1500,\n",
      " 'mean_pixel': [123.68, 116.779, 103.939],\n",
      " 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_openfieldOct30\\\\Documentation_data-openfield_95shuffle1.pickle',\n",
      " 'min_input_size': 64,\n",
      " 'mirror': False,\n",
      " 'multi_step': [[0.005, 10000],\n",
      "                [0.02, 430000],\n",
      "                [0.002, 730000],\n",
      "                [0.001, 1030000]],\n",
      " 'net_type': 'resnet_50',\n",
      " 'num_joints': 4,\n",
      " 'optimizer': 'sgd',\n",
      " 'pairwise_huber_loss': False,\n",
      " 'pairwise_predict': False,\n",
      " 'partaffinityfield_predict': False,\n",
      " 'pos_dist_thresh': 17,\n",
      " 'project_path': 'C:\\\\Users\\\\Public\\\\GitHub\\\\DeepLabCut\\\\examples\\\\openfield-Pranav-2018-10-30',\n",
      " 'regularize': False,\n",
      " 'rotation': 25,\n",
      " 'rotratio': 0.4,\n",
      " 'save_iters': 50000,\n",
      " 'scale_jitter_lo': 0.5,\n",
      " 'scale_jitter_up': 1.25,\n",
      " 'scoremap_dir': 'test',\n",
      " 'shuffle': True,\n",
      " 'snapshot_prefix': 'C:\\\\Users\\\\Public\\\\GitHub\\\\DeepLabCut\\\\examples\\\\openfield-Pranav-2018-10-30\\\\dlc-models\\\\iteration-0\\\\openfieldOct30-trainset95shuffle1\\\\train\\\\snapshot',\n",
      " 'stride': 8.0,\n",
      " 'weigh_negatives': False,\n",
      " 'weigh_only_present_joints': False,\n",
      " 'weigh_part_predictions': False,\n",
      " 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting single-animal trainer\n",
      "Starting with imgaug pose-dataset loader (=default).\n",
      "Batch Size is 1\n",
      "Initializing ResNet\n",
      "Loading ImageNet-pretrained resnet_50\n",
      "Display_iters overwritten as 10\n",
      "Save_iters overwritten as 100\n",
      "Training parameter:\n",
      "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': 'C:\\\\Users\\\\Public\\\\GitHub\\\\DeepLabCut\\\\examples\\\\openfield-Pranav-2018-10-30\\\\dlc-models\\\\iteration-0\\\\openfieldOct30-trainset95shuffle1\\\\train\\\\snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['snout', 'leftear', 'rightear', 'tailbase'], 'alpha_r': 0.02, 'cropratio': 0.4, 'dataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_openfieldOct30\\\\openfield_Pranav95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'init_weights': 'C:\\\\Users\\\\nkinsky\\\\.conda\\\\envs\\\\DLC-GPU\\\\lib\\\\site-packages\\\\deeplabcut\\\\pose_estimation_tensorflow\\\\models\\\\pretrained\\\\resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'metadataset': 'training-datasets\\\\iteration-0\\\\UnaugmentedDataSet_openfieldOct30\\\\Documentation_data-openfield_95shuffle1.pickle', 'min_input_size': 64, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 4, 'pos_dist_thresh': 17, 'project_path': 'C:\\\\Users\\\\Public\\\\GitHub\\\\DeepLabCut\\\\examples\\\\openfield-Pranav-2018-10-30', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': [-90, 90]}}\n",
      "Starting training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 10 loss: 0.2906 lr: 0.005\n",
      "iteration: 20 loss: 0.0692 lr: 0.005\n",
      "iteration: 30 loss: 0.0518 lr: 0.005\n",
      "iteration: 40 loss: 0.0346 lr: 0.005\n",
      "iteration: 50 loss: 0.0383 lr: 0.005\n",
      "iteration: 60 loss: 0.0378 lr: 0.005\n",
      "iteration: 70 loss: 0.0352 lr: 0.005\n",
      "iteration: 80 loss: 0.0272 lr: 0.005\n",
      "iteration: 90 loss: 0.0380 lr: 0.005\n",
      "iteration: 100 loss: 0.0335 lr: 0.005\n",
      "iteration: 110 loss: 0.0379 lr: 0.005\n",
      "iteration: 120 loss: 0.0325 lr: 0.005\n",
      "iteration: 130 loss: 0.0259 lr: 0.005\n",
      "iteration: 140 loss: 0.0293 lr: 0.005\n",
      "iteration: 150 loss: 0.0283 lr: 0.005\n",
      "iteration: 160 loss: 0.0279 lr: 0.005\n",
      "iteration: 170 loss: 0.0275 lr: 0.005\n",
      "iteration: 180 loss: 0.0321 lr: 0.005\n",
      "iteration: 190 loss: 0.0308 lr: 0.005\n",
      "iteration: 200 loss: 0.0271 lr: 0.005\n",
      "iteration: 210 loss: 0.0275 lr: 0.005\n",
      "iteration: 220 loss: 0.0259 lr: 0.005\n",
      "iteration: 230 loss: 0.0285 lr: 0.005\n",
      "iteration: 240 loss: 0.0256 lr: 0.005\n",
      "iteration: 250 loss: 0.0288 lr: 0.005\n",
      "iteration: 260 loss: 0.0257 lr: 0.005\n",
      "iteration: 270 loss: 0.0322 lr: 0.005\n",
      "iteration: 280 loss: 0.0224 lr: 0.005\n",
      "iteration: 290 loss: 0.0290 lr: 0.005\n",
      "iteration: 300 loss: 0.0255 lr: 0.005\n",
      "iteration: 310 loss: 0.0238 lr: 0.005\n",
      "iteration: 320 loss: 0.0239 lr: 0.005\n",
      "iteration: 330 loss: 0.0285 lr: 0.005\n",
      "iteration: 340 loss: 0.0225 lr: 0.005\n",
      "iteration: 350 loss: 0.0214 lr: 0.005\n",
      "iteration: 360 loss: 0.0229 lr: 0.005\n",
      "iteration: 370 loss: 0.0233 lr: 0.005\n",
      "iteration: 380 loss: 0.0242 lr: 0.005\n",
      "iteration: 390 loss: 0.0196 lr: 0.005\n",
      "iteration: 400 loss: 0.0201 lr: 0.005\n",
      "iteration: 410 loss: 0.0191 lr: 0.005\n",
      "iteration: 420 loss: 0.0283 lr: 0.005\n",
      "iteration: 430 loss: 0.0209 lr: 0.005\n",
      "iteration: 440 loss: 0.0203 lr: 0.005\n",
      "iteration: 450 loss: 0.0218 lr: 0.005\n",
      "iteration: 460 loss: 0.0214 lr: 0.005\n",
      "iteration: 470 loss: 0.0196 lr: 0.005\n",
      "iteration: 480 loss: 0.0223 lr: 0.005\n",
      "iteration: 490 loss: 0.0194 lr: 0.005\n",
      "iteration: 500 loss: 0.0210 lr: 0.005\n",
      "iteration: 510 loss: 0.0257 lr: 0.005\n",
      "iteration: 520 loss: 0.0176 lr: 0.005\n",
      "iteration: 530 loss: 0.0201 lr: 0.005\n",
      "iteration: 540 loss: 0.0224 lr: 0.005\n",
      "iteration: 550 loss: 0.0173 lr: 0.005\n",
      "iteration: 560 loss: 0.0191 lr: 0.005\n",
      "iteration: 570 loss: 0.0168 lr: 0.005\n",
      "iteration: 580 loss: 0.0163 lr: 0.005\n",
      "iteration: 590 loss: 0.0202 lr: 0.005\n",
      "iteration: 600 loss: 0.0194 lr: 0.005\n",
      "iteration: 610 loss: 0.0178 lr: 0.005\n",
      "iteration: 620 loss: 0.0203 lr: 0.005\n",
      "iteration: 630 loss: 0.0192 lr: 0.005\n",
      "iteration: 640 loss: 0.0169 lr: 0.005\n",
      "iteration: 650 loss: 0.0217 lr: 0.005\n",
      "iteration: 660 loss: 0.0209 lr: 0.005\n",
      "iteration: 670 loss: 0.0213 lr: 0.005\n",
      "iteration: 680 loss: 0.0223 lr: 0.005\n",
      "iteration: 690 loss: 0.0170 lr: 0.005\n",
      "iteration: 700 loss: 0.0168 lr: 0.005\n",
      "iteration: 710 loss: 0.0173 lr: 0.005\n",
      "iteration: 720 loss: 0.0161 lr: 0.005\n",
      "iteration: 730 loss: 0.0143 lr: 0.005\n",
      "iteration: 740 loss: 0.0124 lr: 0.005\n",
      "iteration: 750 loss: 0.0138 lr: 0.005\n",
      "iteration: 760 loss: 0.0149 lr: 0.005\n",
      "iteration: 770 loss: 0.0149 lr: 0.005\n",
      "iteration: 780 loss: 0.0170 lr: 0.005\n",
      "iteration: 790 loss: 0.0162 lr: 0.005\n",
      "iteration: 800 loss: 0.0163 lr: 0.005\n",
      "iteration: 810 loss: 0.0164 lr: 0.005\n",
      "iteration: 820 loss: 0.0132 lr: 0.005\n",
      "iteration: 830 loss: 0.0161 lr: 0.005\n",
      "iteration: 840 loss: 0.0197 lr: 0.005\n",
      "iteration: 850 loss: 0.0140 lr: 0.005\n",
      "iteration: 860 loss: 0.0126 lr: 0.005\n",
      "iteration: 870 loss: 0.0140 lr: 0.005\n",
      "iteration: 880 loss: 0.0194 lr: 0.005\n",
      "iteration: 890 loss: 0.0192 lr: 0.005\n",
      "iteration: 900 loss: 0.0151 lr: 0.005\n",
      "iteration: 910 loss: 0.0153 lr: 0.005\n",
      "iteration: 920 loss: 0.0136 lr: 0.005\n",
      "iteration: 930 loss: 0.0145 lr: 0.005\n",
      "iteration: 940 loss: 0.0152 lr: 0.005\n",
      "iteration: 950 loss: 0.0131 lr: 0.005\n",
      "iteration: 960 loss: 0.0139 lr: 0.005\n",
      "iteration: 970 loss: 0.0106 lr: 0.005\n",
      "iteration: 980 loss: 0.0165 lr: 0.005\n",
      "iteration: 990 loss: 0.0097 lr: 0.005\n",
      "iteration: 1000 loss: 0.0127 lr: 0.005\n",
      "iteration: 1010 loss: 0.0165 lr: 0.005\n",
      "iteration: 1020 loss: 0.0118 lr: 0.005\n",
      "iteration: 1030 loss: 0.0131 lr: 0.005\n",
      "iteration: 1040 loss: 0.0162 lr: 0.005\n",
      "iteration: 1050 loss: 0.0141 lr: 0.005\n",
      "iteration: 1060 loss: 0.0140 lr: 0.005\n",
      "iteration: 1070 loss: 0.0148 lr: 0.005\n",
      "iteration: 1080 loss: 0.0122 lr: 0.005\n",
      "iteration: 1090 loss: 0.0149 lr: 0.005\n",
      "iteration: 1100 loss: 0.0133 lr: 0.005\n",
      "iteration: 1110 loss: 0.0110 lr: 0.005\n",
      "iteration: 1120 loss: 0.0113 lr: 0.005\n",
      "iteration: 1130 loss: 0.0130 lr: 0.005\n",
      "iteration: 1140 loss: 0.0154 lr: 0.005\n",
      "iteration: 1150 loss: 0.0167 lr: 0.005\n",
      "iteration: 1160 loss: 0.0118 lr: 0.005\n",
      "iteration: 1170 loss: 0.0134 lr: 0.005\n",
      "iteration: 1180 loss: 0.0103 lr: 0.005\n",
      "iteration: 1190 loss: 0.0138 lr: 0.005\n",
      "iteration: 1200 loss: 0.0128 lr: 0.005\n",
      "iteration: 1210 loss: 0.0128 lr: 0.005\n",
      "iteration: 1220 loss: 0.0108 lr: 0.005\n",
      "iteration: 1230 loss: 0.0102 lr: 0.005\n",
      "iteration: 1240 loss: 0.0136 lr: 0.005\n",
      "iteration: 1250 loss: 0.0106 lr: 0.005\n",
      "iteration: 1260 loss: 0.0134 lr: 0.005\n",
      "iteration: 1270 loss: 0.0118 lr: 0.005\n",
      "iteration: 1280 loss: 0.0103 lr: 0.005\n",
      "iteration: 1290 loss: 0.0128 lr: 0.005\n",
      "iteration: 1300 loss: 0.0118 lr: 0.005\n",
      "iteration: 1310 loss: 0.0113 lr: 0.005\n",
      "iteration: 1320 loss: 0.0126 lr: 0.005\n",
      "iteration: 1330 loss: 0.0109 lr: 0.005\n",
      "iteration: 1340 loss: 0.0142 lr: 0.005\n",
      "iteration: 1350 loss: 0.0133 lr: 0.005\n",
      "iteration: 1360 loss: 0.0111 lr: 0.005\n",
      "iteration: 1370 loss: 0.0149 lr: 0.005\n",
      "iteration: 1380 loss: 0.0131 lr: 0.005\n",
      "iteration: 1390 loss: 0.0119 lr: 0.005\n",
      "iteration: 1400 loss: 0.0104 lr: 0.005\n",
      "iteration: 1410 loss: 0.0135 lr: 0.005\n",
      "iteration: 1420 loss: 0.0139 lr: 0.005\n",
      "iteration: 1430 loss: 0.0143 lr: 0.005\n",
      "iteration: 1440 loss: 0.0124 lr: 0.005\n",
      "iteration: 1450 loss: 0.0122 lr: 0.005\n",
      "iteration: 1460 loss: 0.0169 lr: 0.005\n",
      "iteration: 1470 loss: 0.0109 lr: 0.005\n",
      "iteration: 1480 loss: 0.0147 lr: 0.005\n",
      "iteration: 1490 loss: 0.0102 lr: 0.005\n",
      "iteration: 1500 loss: 0.0108 lr: 0.005\n",
      "iteration: 1510 loss: 0.0091 lr: 0.005\n",
      "iteration: 1520 loss: 0.0129 lr: 0.005\n",
      "iteration: 1530 loss: 0.0098 lr: 0.005\n",
      "iteration: 1540 loss: 0.0091 lr: 0.005\n",
      "iteration: 1550 loss: 0.0106 lr: 0.005\n",
      "iteration: 1560 loss: 0.0085 lr: 0.005\n",
      "iteration: 1570 loss: 0.0104 lr: 0.005\n",
      "iteration: 1580 loss: 0.0116 lr: 0.005\n",
      "iteration: 1590 loss: 0.0152 lr: 0.005\n",
      "iteration: 1600 loss: 0.0110 lr: 0.005\n",
      "iteration: 1610 loss: 0.0113 lr: 0.005\n",
      "iteration: 1620 loss: 0.0115 lr: 0.005\n",
      "iteration: 1630 loss: 0.0074 lr: 0.005\n",
      "iteration: 1640 loss: 0.0106 lr: 0.005\n",
      "iteration: 1650 loss: 0.0116 lr: 0.005\n",
      "iteration: 1660 loss: 0.0107 lr: 0.005\n",
      "iteration: 1670 loss: 0.0076 lr: 0.005\n",
      "iteration: 1680 loss: 0.0102 lr: 0.005\n",
      "iteration: 1690 loss: 0.0115 lr: 0.005\n",
      "iteration: 1700 loss: 0.0101 lr: 0.005\n",
      "iteration: 1710 loss: 0.0113 lr: 0.005\n",
      "iteration: 1720 loss: 0.0107 lr: 0.005\n",
      "iteration: 1730 loss: 0.0102 lr: 0.005\n",
      "iteration: 1740 loss: 0.0105 lr: 0.005\n",
      "iteration: 1750 loss: 0.0125 lr: 0.005\n",
      "iteration: 1760 loss: 0.0088 lr: 0.005\n",
      "iteration: 1770 loss: 0.0093 lr: 0.005\n",
      "iteration: 1780 loss: 0.0106 lr: 0.005\n",
      "iteration: 1790 loss: 0.0074 lr: 0.005\n",
      "iteration: 1800 loss: 0.0121 lr: 0.005\n",
      "iteration: 1810 loss: 0.0091 lr: 0.005\n",
      "iteration: 1820 loss: 0.0135 lr: 0.005\n",
      "iteration: 1830 loss: 0.0081 lr: 0.005\n",
      "iteration: 1840 loss: 0.0080 lr: 0.005\n",
      "iteration: 1850 loss: 0.0103 lr: 0.005\n",
      "iteration: 1860 loss: 0.0097 lr: 0.005\n",
      "iteration: 1870 loss: 0.0105 lr: 0.005\n",
      "iteration: 1880 loss: 0.0103 lr: 0.005\n",
      "iteration: 1890 loss: 0.0100 lr: 0.005\n",
      "iteration: 1900 loss: 0.0135 lr: 0.005\n",
      "iteration: 1910 loss: 0.0104 lr: 0.005\n",
      "iteration: 1920 loss: 0.0099 lr: 0.005\n",
      "iteration: 1930 loss: 0.0119 lr: 0.005\n",
      "iteration: 1940 loss: 0.0092 lr: 0.005\n",
      "iteration: 1950 loss: 0.0081 lr: 0.005\n",
      "iteration: 1960 loss: 0.0089 lr: 0.005\n",
      "iteration: 1970 loss: 0.0071 lr: 0.005\n",
      "iteration: 1980 loss: 0.0092 lr: 0.005\n",
      "iteration: 1990 loss: 0.0134 lr: 0.005\n",
      "iteration: 2000 loss: 0.0090 lr: 0.005\n",
      "iteration: 2010 loss: 0.0088 lr: 0.005\n",
      "iteration: 2020 loss: 0.0073 lr: 0.005\n",
      "iteration: 2030 loss: 0.0098 lr: 0.005\n",
      "iteration: 2040 loss: 0.0110 lr: 0.005\n",
      "iteration: 2050 loss: 0.0080 lr: 0.005\n",
      "iteration: 2060 loss: 0.0100 lr: 0.005\n",
      "iteration: 2070 loss: 0.0122 lr: 0.005\n",
      "iteration: 2080 loss: 0.0111 lr: 0.005\n",
      "iteration: 2090 loss: 0.0124 lr: 0.005\n",
      "iteration: 2100 loss: 0.0095 lr: 0.005\n",
      "iteration: 2110 loss: 0.0126 lr: 0.005\n",
      "iteration: 2120 loss: 0.0093 lr: 0.005\n",
      "iteration: 2130 loss: 0.0086 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 2140 loss: 0.0080 lr: 0.005\n",
      "iteration: 2150 loss: 0.0079 lr: 0.005\n",
      "iteration: 2160 loss: 0.0106 lr: 0.005\n",
      "iteration: 2170 loss: 0.0099 lr: 0.005\n",
      "iteration: 2180 loss: 0.0097 lr: 0.005\n",
      "iteration: 2190 loss: 0.0097 lr: 0.005\n",
      "iteration: 2200 loss: 0.0068 lr: 0.005\n",
      "iteration: 2210 loss: 0.0098 lr: 0.005\n",
      "iteration: 2220 loss: 0.0089 lr: 0.005\n",
      "iteration: 2230 loss: 0.0103 lr: 0.005\n",
      "iteration: 2240 loss: 0.0199 lr: 0.005\n",
      "iteration: 2250 loss: 0.0102 lr: 0.005\n",
      "iteration: 2260 loss: 0.0069 lr: 0.005\n",
      "iteration: 2270 loss: 0.0108 lr: 0.005\n",
      "iteration: 2280 loss: 0.0079 lr: 0.005\n",
      "iteration: 2290 loss: 0.0083 lr: 0.005\n",
      "iteration: 2300 loss: 0.0094 lr: 0.005\n",
      "iteration: 2310 loss: 0.0088 lr: 0.005\n",
      "iteration: 2320 loss: 0.0078 lr: 0.005\n",
      "iteration: 2330 loss: 0.0117 lr: 0.005\n",
      "iteration: 2340 loss: 0.0084 lr: 0.005\n",
      "iteration: 2350 loss: 0.0104 lr: 0.005\n",
      "iteration: 2360 loss: 0.0099 lr: 0.005\n",
      "iteration: 2370 loss: 0.0132 lr: 0.005\n",
      "iteration: 2380 loss: 0.0140 lr: 0.005\n",
      "iteration: 2390 loss: 0.0116 lr: 0.005\n",
      "iteration: 2400 loss: 0.0131 lr: 0.005\n",
      "iteration: 2410 loss: 0.0097 lr: 0.005\n",
      "iteration: 2420 loss: 0.0085 lr: 0.005\n",
      "iteration: 2430 loss: 0.0089 lr: 0.005\n",
      "iteration: 2440 loss: 0.0086 lr: 0.005\n",
      "iteration: 2450 loss: 0.0087 lr: 0.005\n",
      "iteration: 2460 loss: 0.0084 lr: 0.005\n",
      "iteration: 2470 loss: 0.0085 lr: 0.005\n",
      "iteration: 2480 loss: 0.0089 lr: 0.005\n",
      "iteration: 2490 loss: 0.0110 lr: 0.005\n",
      "iteration: 2500 loss: 0.0086 lr: 0.005\n",
      "iteration: 2510 loss: 0.0072 lr: 0.005\n",
      "iteration: 2520 loss: 0.0093 lr: 0.005\n",
      "iteration: 2530 loss: 0.0134 lr: 0.005\n",
      "iteration: 2540 loss: 0.0096 lr: 0.005\n",
      "iteration: 2550 loss: 0.0072 lr: 0.005\n",
      "iteration: 2560 loss: 0.0083 lr: 0.005\n",
      "iteration: 2570 loss: 0.0101 lr: 0.005\n",
      "iteration: 2580 loss: 0.0102 lr: 0.005\n",
      "iteration: 2590 loss: 0.0075 lr: 0.005\n",
      "iteration: 2600 loss: 0.0081 lr: 0.005\n",
      "iteration: 2610 loss: 0.0094 lr: 0.005\n",
      "iteration: 2620 loss: 0.0069 lr: 0.005\n",
      "iteration: 2630 loss: 0.0080 lr: 0.005\n",
      "iteration: 2640 loss: 0.0105 lr: 0.005\n",
      "iteration: 2650 loss: 0.0072 lr: 0.005\n",
      "iteration: 2660 loss: 0.0121 lr: 0.005\n",
      "iteration: 2670 loss: 0.0117 lr: 0.005\n",
      "iteration: 2680 loss: 0.0100 lr: 0.005\n",
      "iteration: 2690 loss: 0.0070 lr: 0.005\n",
      "iteration: 2700 loss: 0.0092 lr: 0.005\n",
      "iteration: 2710 loss: 0.0069 lr: 0.005\n",
      "iteration: 2720 loss: 0.0099 lr: 0.005\n",
      "iteration: 2730 loss: 0.0124 lr: 0.005\n",
      "iteration: 2740 loss: 0.0088 lr: 0.005\n",
      "iteration: 2750 loss: 0.0073 lr: 0.005\n",
      "iteration: 2760 loss: 0.0068 lr: 0.005\n",
      "iteration: 2770 loss: 0.0094 lr: 0.005\n",
      "iteration: 2780 loss: 0.0077 lr: 0.005\n",
      "iteration: 2790 loss: 0.0074 lr: 0.005\n",
      "iteration: 2800 loss: 0.0101 lr: 0.005\n",
      "iteration: 2810 loss: 0.0098 lr: 0.005\n",
      "iteration: 2820 loss: 0.0078 lr: 0.005\n",
      "iteration: 2830 loss: 0.0094 lr: 0.005\n",
      "iteration: 2840 loss: 0.0075 lr: 0.005\n",
      "iteration: 2850 loss: 0.0071 lr: 0.005\n",
      "iteration: 2860 loss: 0.0073 lr: 0.005\n",
      "iteration: 2870 loss: 0.0079 lr: 0.005\n",
      "iteration: 2880 loss: 0.0081 lr: 0.005\n",
      "iteration: 2890 loss: 0.0091 lr: 0.005\n",
      "iteration: 2900 loss: 0.0071 lr: 0.005\n",
      "iteration: 2910 loss: 0.0085 lr: 0.005\n",
      "iteration: 2920 loss: 0.0066 lr: 0.005\n",
      "iteration: 2930 loss: 0.0085 lr: 0.005\n",
      "iteration: 2940 loss: 0.0076 lr: 0.005\n",
      "iteration: 2950 loss: 0.0083 lr: 0.005\n",
      "iteration: 2960 loss: 0.0099 lr: 0.005\n",
      "iteration: 2970 loss: 0.0078 lr: 0.005\n",
      "iteration: 2980 loss: 0.0093 lr: 0.005\n",
      "iteration: 2990 loss: 0.0062 lr: 0.005\n",
      "iteration: 3000 loss: 0.0084 lr: 0.005\n",
      "iteration: 3010 loss: 0.0087 lr: 0.005\n",
      "iteration: 3020 loss: 0.0076 lr: 0.005\n",
      "iteration: 3030 loss: 0.0085 lr: 0.005\n",
      "iteration: 3040 loss: 0.0062 lr: 0.005\n",
      "iteration: 3050 loss: 0.0079 lr: 0.005\n",
      "iteration: 3060 loss: 0.0060 lr: 0.005\n",
      "iteration: 3070 loss: 0.0118 lr: 0.005\n",
      "iteration: 3080 loss: 0.0075 lr: 0.005\n",
      "iteration: 3090 loss: 0.0166 lr: 0.005\n",
      "iteration: 3100 loss: 0.0097 lr: 0.005\n",
      "iteration: 3110 loss: 0.0077 lr: 0.005\n",
      "iteration: 3120 loss: 0.0093 lr: 0.005\n",
      "iteration: 3130 loss: 0.0080 lr: 0.005\n",
      "iteration: 3140 loss: 0.0094 lr: 0.005\n",
      "iteration: 3150 loss: 0.0082 lr: 0.005\n",
      "iteration: 3160 loss: 0.0110 lr: 0.005\n",
      "iteration: 3170 loss: 0.0080 lr: 0.005\n",
      "iteration: 3180 loss: 0.0115 lr: 0.005\n",
      "iteration: 3190 loss: 0.0091 lr: 0.005\n",
      "iteration: 3200 loss: 0.0082 lr: 0.005\n",
      "iteration: 3210 loss: 0.0073 lr: 0.005\n",
      "iteration: 3220 loss: 0.0079 lr: 0.005\n",
      "iteration: 3230 loss: 0.0064 lr: 0.005\n",
      "iteration: 3240 loss: 0.0082 lr: 0.005\n",
      "iteration: 3250 loss: 0.0068 lr: 0.005\n",
      "iteration: 3260 loss: 0.0076 lr: 0.005\n",
      "iteration: 3270 loss: 0.0063 lr: 0.005\n",
      "iteration: 3280 loss: 0.0074 lr: 0.005\n",
      "iteration: 3290 loss: 0.0075 lr: 0.005\n",
      "iteration: 3300 loss: 0.0085 lr: 0.005\n",
      "iteration: 3310 loss: 0.0069 lr: 0.005\n",
      "iteration: 3320 loss: 0.0084 lr: 0.005\n",
      "iteration: 3330 loss: 0.0105 lr: 0.005\n",
      "iteration: 3340 loss: 0.0060 lr: 0.005\n",
      "iteration: 3350 loss: 0.0089 lr: 0.005\n",
      "iteration: 3360 loss: 0.0070 lr: 0.005\n",
      "iteration: 3370 loss: 0.0075 lr: 0.005\n",
      "iteration: 3380 loss: 0.0081 lr: 0.005\n",
      "iteration: 3390 loss: 0.0108 lr: 0.005\n",
      "iteration: 3400 loss: 0.0083 lr: 0.005\n",
      "iteration: 3410 loss: 0.0086 lr: 0.005\n",
      "iteration: 3420 loss: 0.0092 lr: 0.005\n",
      "iteration: 3430 loss: 0.0099 lr: 0.005\n",
      "iteration: 3440 loss: 0.0072 lr: 0.005\n",
      "iteration: 3450 loss: 0.0077 lr: 0.005\n",
      "iteration: 3460 loss: 0.0079 lr: 0.005\n",
      "iteration: 3470 loss: 0.0082 lr: 0.005\n",
      "iteration: 3480 loss: 0.0090 lr: 0.005\n",
      "iteration: 3490 loss: 0.0097 lr: 0.005\n",
      "iteration: 3500 loss: 0.0070 lr: 0.005\n",
      "iteration: 3510 loss: 0.0095 lr: 0.005\n",
      "iteration: 3520 loss: 0.0081 lr: 0.005\n",
      "iteration: 3530 loss: 0.0085 lr: 0.005\n",
      "iteration: 3540 loss: 0.0066 lr: 0.005\n",
      "iteration: 3550 loss: 0.0062 lr: 0.005\n",
      "iteration: 3560 loss: 0.0076 lr: 0.005\n",
      "iteration: 3570 loss: 0.0059 lr: 0.005\n",
      "iteration: 3580 loss: 0.0066 lr: 0.005\n",
      "iteration: 3590 loss: 0.0080 lr: 0.005\n",
      "iteration: 3600 loss: 0.0080 lr: 0.005\n",
      "iteration: 3610 loss: 0.0088 lr: 0.005\n",
      "iteration: 3620 loss: 0.0072 lr: 0.005\n",
      "iteration: 3630 loss: 0.0076 lr: 0.005\n",
      "iteration: 3640 loss: 0.0070 lr: 0.005\n",
      "iteration: 3650 loss: 0.0063 lr: 0.005\n",
      "iteration: 3660 loss: 0.0062 lr: 0.005\n",
      "iteration: 3670 loss: 0.0055 lr: 0.005\n",
      "iteration: 3680 loss: 0.0066 lr: 0.005\n",
      "iteration: 3690 loss: 0.0092 lr: 0.005\n",
      "iteration: 3700 loss: 0.0081 lr: 0.005\n",
      "iteration: 3710 loss: 0.0122 lr: 0.005\n",
      "iteration: 3720 loss: 0.0076 lr: 0.005\n",
      "iteration: 3730 loss: 0.0092 lr: 0.005\n",
      "iteration: 3740 loss: 0.0082 lr: 0.005\n",
      "iteration: 3750 loss: 0.0069 lr: 0.005\n",
      "iteration: 3760 loss: 0.0071 lr: 0.005\n",
      "iteration: 3770 loss: 0.0092 lr: 0.005\n",
      "iteration: 3780 loss: 0.0084 lr: 0.005\n",
      "iteration: 3790 loss: 0.0086 lr: 0.005\n",
      "iteration: 3800 loss: 0.0079 lr: 0.005\n",
      "iteration: 3810 loss: 0.0064 lr: 0.005\n",
      "iteration: 3820 loss: 0.0096 lr: 0.005\n",
      "iteration: 3830 loss: 0.0081 lr: 0.005\n",
      "iteration: 3840 loss: 0.0072 lr: 0.005\n",
      "iteration: 3850 loss: 0.0082 lr: 0.005\n",
      "iteration: 3860 loss: 0.0059 lr: 0.005\n",
      "iteration: 3870 loss: 0.0055 lr: 0.005\n",
      "iteration: 3880 loss: 0.0090 lr: 0.005\n",
      "iteration: 3890 loss: 0.0079 lr: 0.005\n",
      "iteration: 3900 loss: 0.0060 lr: 0.005\n",
      "iteration: 3910 loss: 0.0069 lr: 0.005\n",
      "iteration: 3920 loss: 0.0065 lr: 0.005\n",
      "iteration: 3930 loss: 0.0074 lr: 0.005\n",
      "iteration: 3940 loss: 0.0090 lr: 0.005\n",
      "iteration: 3950 loss: 0.0083 lr: 0.005\n",
      "iteration: 3960 loss: 0.0096 lr: 0.005\n",
      "iteration: 3970 loss: 0.0065 lr: 0.005\n",
      "iteration: 3980 loss: 0.0080 lr: 0.005\n",
      "iteration: 3990 loss: 0.0107 lr: 0.005\n",
      "iteration: 4000 loss: 0.0075 lr: 0.005\n",
      "iteration: 4010 loss: 0.0072 lr: 0.005\n",
      "iteration: 4020 loss: 0.0063 lr: 0.005\n",
      "iteration: 4030 loss: 0.0071 lr: 0.005\n",
      "iteration: 4040 loss: 0.0073 lr: 0.005\n",
      "iteration: 4050 loss: 0.0069 lr: 0.005\n",
      "iteration: 4060 loss: 0.0105 lr: 0.005\n",
      "iteration: 4070 loss: 0.0088 lr: 0.005\n",
      "iteration: 4080 loss: 0.0071 lr: 0.005\n",
      "iteration: 4090 loss: 0.0106 lr: 0.005\n",
      "iteration: 4100 loss: 0.0083 lr: 0.005\n",
      "iteration: 4110 loss: 0.0071 lr: 0.005\n",
      "iteration: 4120 loss: 0.0083 lr: 0.005\n",
      "iteration: 4130 loss: 0.0072 lr: 0.005\n",
      "iteration: 4140 loss: 0.0091 lr: 0.005\n",
      "iteration: 4150 loss: 0.0075 lr: 0.005\n",
      "iteration: 4160 loss: 0.0075 lr: 0.005\n",
      "iteration: 4170 loss: 0.0055 lr: 0.005\n",
      "iteration: 4180 loss: 0.0060 lr: 0.005\n",
      "iteration: 4190 loss: 0.0074 lr: 0.005\n",
      "iteration: 4200 loss: 0.0090 lr: 0.005\n",
      "iteration: 4210 loss: 0.0099 lr: 0.005\n",
      "iteration: 4220 loss: 0.0084 lr: 0.005\n",
      "iteration: 4230 loss: 0.0104 lr: 0.005\n",
      "iteration: 4240 loss: 0.0064 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 4250 loss: 0.0091 lr: 0.005\n",
      "iteration: 4260 loss: 0.0059 lr: 0.005\n",
      "iteration: 4270 loss: 0.0060 lr: 0.005\n",
      "iteration: 4280 loss: 0.0107 lr: 0.005\n",
      "iteration: 4290 loss: 0.0069 lr: 0.005\n",
      "iteration: 4300 loss: 0.0071 lr: 0.005\n",
      "iteration: 4310 loss: 0.0075 lr: 0.005\n",
      "iteration: 4320 loss: 0.0065 lr: 0.005\n",
      "iteration: 4330 loss: 0.0071 lr: 0.005\n",
      "iteration: 4340 loss: 0.0078 lr: 0.005\n",
      "iteration: 4350 loss: 0.0083 lr: 0.005\n",
      "iteration: 4360 loss: 0.0061 lr: 0.005\n",
      "iteration: 4370 loss: 0.0070 lr: 0.005\n",
      "iteration: 4380 loss: 0.0058 lr: 0.005\n",
      "iteration: 4390 loss: 0.0073 lr: 0.005\n",
      "iteration: 4400 loss: 0.0056 lr: 0.005\n",
      "iteration: 4410 loss: 0.0059 lr: 0.005\n",
      "iteration: 4420 loss: 0.0065 lr: 0.005\n",
      "iteration: 4430 loss: 0.0071 lr: 0.005\n",
      "iteration: 4440 loss: 0.0068 lr: 0.005\n",
      "iteration: 4450 loss: 0.0072 lr: 0.005\n",
      "iteration: 4460 loss: 0.0100 lr: 0.005\n",
      "iteration: 4470 loss: 0.0074 lr: 0.005\n",
      "iteration: 4480 loss: 0.0072 lr: 0.005\n",
      "iteration: 4490 loss: 0.0058 lr: 0.005\n",
      "iteration: 4500 loss: 0.0073 lr: 0.005\n",
      "iteration: 4510 loss: 0.0066 lr: 0.005\n",
      "iteration: 4520 loss: 0.0091 lr: 0.005\n",
      "iteration: 4530 loss: 0.0059 lr: 0.005\n",
      "iteration: 4540 loss: 0.0086 lr: 0.005\n",
      "iteration: 4550 loss: 0.0086 lr: 0.005\n",
      "iteration: 4560 loss: 0.0095 lr: 0.005\n",
      "iteration: 4570 loss: 0.0087 lr: 0.005\n",
      "iteration: 4580 loss: 0.0078 lr: 0.005\n",
      "iteration: 4590 loss: 0.0071 lr: 0.005\n",
      "iteration: 4600 loss: 0.0071 lr: 0.005\n",
      "iteration: 4610 loss: 0.0075 lr: 0.005\n",
      "iteration: 4620 loss: 0.0084 lr: 0.005\n",
      "iteration: 4630 loss: 0.0071 lr: 0.005\n",
      "iteration: 4640 loss: 0.0059 lr: 0.005\n",
      "iteration: 4650 loss: 0.0113 lr: 0.005\n",
      "iteration: 4660 loss: 0.0081 lr: 0.005\n",
      "iteration: 4670 loss: 0.0067 lr: 0.005\n",
      "iteration: 4680 loss: 0.0057 lr: 0.005\n",
      "iteration: 4690 loss: 0.0060 lr: 0.005\n",
      "iteration: 4700 loss: 0.0099 lr: 0.005\n",
      "iteration: 4710 loss: 0.0059 lr: 0.005\n",
      "iteration: 4720 loss: 0.0056 lr: 0.005\n",
      "iteration: 4730 loss: 0.0064 lr: 0.005\n",
      "iteration: 4740 loss: 0.0065 lr: 0.005\n",
      "iteration: 4750 loss: 0.0050 lr: 0.005\n",
      "iteration: 4760 loss: 0.0069 lr: 0.005\n",
      "iteration: 4770 loss: 0.0063 lr: 0.005\n",
      "iteration: 4780 loss: 0.0090 lr: 0.005\n",
      "iteration: 4790 loss: 0.0092 lr: 0.005\n",
      "iteration: 4800 loss: 0.0096 lr: 0.005\n",
      "iteration: 4810 loss: 0.0094 lr: 0.005\n",
      "iteration: 4820 loss: 0.0102 lr: 0.005\n",
      "iteration: 4830 loss: 0.0089 lr: 0.005\n",
      "iteration: 4840 loss: 0.0064 lr: 0.005\n",
      "iteration: 4850 loss: 0.0066 lr: 0.005\n",
      "iteration: 4860 loss: 0.0069 lr: 0.005\n",
      "iteration: 4870 loss: 0.0094 lr: 0.005\n",
      "iteration: 4880 loss: 0.0102 lr: 0.005\n",
      "iteration: 4890 loss: 0.0075 lr: 0.005\n",
      "iteration: 4900 loss: 0.0061 lr: 0.005\n",
      "iteration: 4910 loss: 0.0077 lr: 0.005\n",
      "iteration: 4920 loss: 0.0062 lr: 0.005\n",
      "iteration: 4930 loss: 0.0061 lr: 0.005\n",
      "iteration: 4940 loss: 0.0062 lr: 0.005\n",
      "iteration: 4950 loss: 0.0049 lr: 0.005\n",
      "iteration: 4960 loss: 0.0070 lr: 0.005\n",
      "iteration: 4970 loss: 0.0056 lr: 0.005\n",
      "iteration: 4980 loss: 0.0062 lr: 0.005\n",
      "iteration: 4990 loss: 0.0063 lr: 0.005\n",
      "iteration: 5000 loss: 0.0075 lr: 0.005\n",
      "iteration: 5010 loss: 0.0082 lr: 0.005\n",
      "iteration: 5020 loss: 0.0080 lr: 0.005\n",
      "iteration: 5030 loss: 0.0062 lr: 0.005\n",
      "iteration: 5040 loss: 0.0085 lr: 0.005\n",
      "iteration: 5050 loss: 0.0070 lr: 0.005\n",
      "iteration: 5060 loss: 0.0078 lr: 0.005\n",
      "iteration: 5070 loss: 0.0079 lr: 0.005\n",
      "iteration: 5080 loss: 0.0071 lr: 0.005\n",
      "iteration: 5090 loss: 0.0065 lr: 0.005\n",
      "iteration: 5100 loss: 0.0061 lr: 0.005\n",
      "iteration: 5110 loss: 0.0084 lr: 0.005\n",
      "iteration: 5120 loss: 0.0067 lr: 0.005\n",
      "iteration: 5130 loss: 0.0085 lr: 0.005\n",
      "iteration: 5140 loss: 0.0056 lr: 0.005\n",
      "iteration: 5150 loss: 0.0081 lr: 0.005\n",
      "iteration: 5160 loss: 0.0063 lr: 0.005\n",
      "iteration: 5170 loss: 0.0070 lr: 0.005\n",
      "iteration: 5180 loss: 0.0060 lr: 0.005\n",
      "iteration: 5190 loss: 0.0066 lr: 0.005\n",
      "iteration: 5200 loss: 0.0061 lr: 0.005\n",
      "iteration: 5210 loss: 0.0062 lr: 0.005\n",
      "iteration: 5220 loss: 0.0051 lr: 0.005\n",
      "iteration: 5230 loss: 0.0056 lr: 0.005\n",
      "iteration: 5240 loss: 0.0075 lr: 0.005\n",
      "iteration: 5250 loss: 0.0068 lr: 0.005\n",
      "iteration: 5260 loss: 0.0060 lr: 0.005\n",
      "iteration: 5270 loss: 0.0068 lr: 0.005\n",
      "iteration: 5280 loss: 0.0067 lr: 0.005\n",
      "iteration: 5290 loss: 0.0091 lr: 0.005\n",
      "iteration: 5300 loss: 0.0079 lr: 0.005\n",
      "iteration: 5310 loss: 0.0065 lr: 0.005\n",
      "iteration: 5320 loss: 0.0076 lr: 0.005\n",
      "iteration: 5330 loss: 0.0083 lr: 0.005\n",
      "iteration: 5340 loss: 0.0076 lr: 0.005\n",
      "iteration: 5350 loss: 0.0062 lr: 0.005\n",
      "iteration: 5360 loss: 0.0062 lr: 0.005\n",
      "iteration: 5370 loss: 0.0069 lr: 0.005\n",
      "iteration: 5380 loss: 0.0050 lr: 0.005\n",
      "iteration: 5390 loss: 0.0083 lr: 0.005\n",
      "iteration: 5400 loss: 0.0072 lr: 0.005\n",
      "iteration: 5410 loss: 0.0072 lr: 0.005\n",
      "iteration: 5420 loss: 0.0098 lr: 0.005\n",
      "iteration: 5430 loss: 0.0064 lr: 0.005\n",
      "iteration: 5440 loss: 0.0057 lr: 0.005\n",
      "iteration: 5450 loss: 0.0057 lr: 0.005\n",
      "iteration: 5460 loss: 0.0061 lr: 0.005\n",
      "iteration: 5470 loss: 0.0077 lr: 0.005\n",
      "iteration: 5480 loss: 0.0088 lr: 0.005\n",
      "iteration: 5490 loss: 0.0060 lr: 0.005\n",
      "iteration: 5500 loss: 0.0057 lr: 0.005\n",
      "iteration: 5510 loss: 0.0049 lr: 0.005\n",
      "iteration: 5520 loss: 0.0083 lr: 0.005\n",
      "iteration: 5530 loss: 0.0075 lr: 0.005\n",
      "iteration: 5540 loss: 0.0060 lr: 0.005\n",
      "iteration: 5550 loss: 0.0068 lr: 0.005\n",
      "iteration: 5560 loss: 0.0061 lr: 0.005\n",
      "iteration: 5570 loss: 0.0052 lr: 0.005\n",
      "iteration: 5580 loss: 0.0070 lr: 0.005\n",
      "iteration: 5590 loss: 0.0083 lr: 0.005\n",
      "iteration: 5600 loss: 0.0057 lr: 0.005\n",
      "iteration: 5610 loss: 0.0068 lr: 0.005\n",
      "iteration: 5620 loss: 0.0061 lr: 0.005\n",
      "iteration: 5630 loss: 0.0061 lr: 0.005\n",
      "iteration: 5640 loss: 0.0052 lr: 0.005\n",
      "iteration: 5650 loss: 0.0052 lr: 0.005\n",
      "iteration: 5660 loss: 0.0092 lr: 0.005\n",
      "iteration: 5670 loss: 0.0059 lr: 0.005\n",
      "iteration: 5680 loss: 0.0068 lr: 0.005\n",
      "iteration: 5690 loss: 0.0068 lr: 0.005\n",
      "iteration: 5700 loss: 0.0070 lr: 0.005\n",
      "iteration: 5710 loss: 0.0092 lr: 0.005\n",
      "iteration: 5720 loss: 0.0054 lr: 0.005\n",
      "iteration: 5730 loss: 0.0057 lr: 0.005\n",
      "iteration: 5740 loss: 0.0059 lr: 0.005\n",
      "iteration: 5750 loss: 0.0078 lr: 0.005\n",
      "iteration: 5760 loss: 0.0061 lr: 0.005\n",
      "iteration: 5770 loss: 0.0064 lr: 0.005\n",
      "iteration: 5780 loss: 0.0066 lr: 0.005\n",
      "iteration: 5790 loss: 0.0057 lr: 0.005\n",
      "iteration: 5800 loss: 0.0055 lr: 0.005\n",
      "iteration: 5810 loss: 0.0072 lr: 0.005\n",
      "iteration: 5820 loss: 0.0059 lr: 0.005\n",
      "iteration: 5830 loss: 0.0081 lr: 0.005\n",
      "iteration: 5840 loss: 0.0079 lr: 0.005\n",
      "iteration: 5850 loss: 0.0055 lr: 0.005\n",
      "iteration: 5860 loss: 0.0063 lr: 0.005\n",
      "iteration: 5870 loss: 0.0066 lr: 0.005\n",
      "iteration: 5880 loss: 0.0060 lr: 0.005\n",
      "iteration: 5890 loss: 0.0087 lr: 0.005\n",
      "iteration: 5900 loss: 0.0054 lr: 0.005\n",
      "iteration: 5910 loss: 0.0079 lr: 0.005\n",
      "iteration: 5920 loss: 0.0060 lr: 0.005\n",
      "iteration: 5930 loss: 0.0109 lr: 0.005\n",
      "iteration: 5940 loss: 0.0087 lr: 0.005\n",
      "iteration: 5950 loss: 0.0065 lr: 0.005\n",
      "iteration: 5960 loss: 0.0059 lr: 0.005\n",
      "iteration: 5970 loss: 0.0082 lr: 0.005\n",
      "iteration: 5980 loss: 0.0069 lr: 0.005\n",
      "iteration: 5990 loss: 0.0082 lr: 0.005\n",
      "iteration: 6000 loss: 0.0075 lr: 0.005\n",
      "iteration: 6010 loss: 0.0057 lr: 0.005\n",
      "iteration: 6020 loss: 0.0069 lr: 0.005\n",
      "iteration: 6030 loss: 0.0073 lr: 0.005\n",
      "iteration: 6040 loss: 0.0062 lr: 0.005\n",
      "iteration: 6050 loss: 0.0058 lr: 0.005\n",
      "iteration: 6060 loss: 0.0069 lr: 0.005\n",
      "iteration: 6070 loss: 0.0086 lr: 0.005\n",
      "iteration: 6080 loss: 0.0061 lr: 0.005\n",
      "iteration: 6090 loss: 0.0061 lr: 0.005\n",
      "iteration: 6100 loss: 0.0050 lr: 0.005\n",
      "iteration: 6110 loss: 0.0044 lr: 0.005\n",
      "iteration: 6120 loss: 0.0063 lr: 0.005\n",
      "iteration: 6130 loss: 0.0054 lr: 0.005\n",
      "iteration: 6140 loss: 0.0082 lr: 0.005\n",
      "iteration: 6150 loss: 0.0051 lr: 0.005\n",
      "iteration: 6160 loss: 0.0059 lr: 0.005\n",
      "iteration: 6170 loss: 0.0072 lr: 0.005\n",
      "iteration: 6180 loss: 0.0085 lr: 0.005\n",
      "iteration: 6190 loss: 0.0078 lr: 0.005\n",
      "iteration: 6200 loss: 0.0071 lr: 0.005\n",
      "iteration: 6210 loss: 0.0058 lr: 0.005\n",
      "iteration: 6220 loss: 0.0076 lr: 0.005\n",
      "iteration: 6230 loss: 0.0056 lr: 0.005\n",
      "iteration: 6240 loss: 0.0111 lr: 0.005\n",
      "iteration: 6250 loss: 0.0047 lr: 0.005\n",
      "iteration: 6260 loss: 0.0071 lr: 0.005\n",
      "iteration: 6270 loss: 0.0058 lr: 0.005\n",
      "iteration: 6280 loss: 0.0045 lr: 0.005\n",
      "iteration: 6290 loss: 0.0061 lr: 0.005\n",
      "iteration: 6300 loss: 0.0066 lr: 0.005\n",
      "iteration: 6310 loss: 0.0066 lr: 0.005\n",
      "iteration: 6320 loss: 0.0060 lr: 0.005\n",
      "iteration: 6330 loss: 0.0080 lr: 0.005\n",
      "iteration: 6340 loss: 0.0050 lr: 0.005\n",
      "iteration: 6350 loss: 0.0071 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 6360 loss: 0.0067 lr: 0.005\n",
      "iteration: 6370 loss: 0.0066 lr: 0.005\n",
      "iteration: 6380 loss: 0.0062 lr: 0.005\n",
      "iteration: 6390 loss: 0.0075 lr: 0.005\n",
      "iteration: 6400 loss: 0.0060 lr: 0.005\n",
      "iteration: 6410 loss: 0.0062 lr: 0.005\n",
      "iteration: 6420 loss: 0.0070 lr: 0.005\n",
      "iteration: 6430 loss: 0.0059 lr: 0.005\n",
      "iteration: 6440 loss: 0.0058 lr: 0.005\n",
      "iteration: 6450 loss: 0.0058 lr: 0.005\n",
      "iteration: 6460 loss: 0.0061 lr: 0.005\n",
      "iteration: 6470 loss: 0.0058 lr: 0.005\n",
      "iteration: 6480 loss: 0.0045 lr: 0.005\n",
      "iteration: 6490 loss: 0.0053 lr: 0.005\n",
      "iteration: 6500 loss: 0.0047 lr: 0.005\n",
      "iteration: 6510 loss: 0.0087 lr: 0.005\n",
      "iteration: 6520 loss: 0.0073 lr: 0.005\n",
      "iteration: 6530 loss: 0.0051 lr: 0.005\n",
      "iteration: 6540 loss: 0.0061 lr: 0.005\n",
      "iteration: 6550 loss: 0.0060 lr: 0.005\n",
      "iteration: 6560 loss: 0.0045 lr: 0.005\n",
      "iteration: 6570 loss: 0.0065 lr: 0.005\n",
      "iteration: 6580 loss: 0.0054 lr: 0.005\n",
      "iteration: 6590 loss: 0.0057 lr: 0.005\n",
      "iteration: 6600 loss: 0.0080 lr: 0.005\n",
      "iteration: 6610 loss: 0.0060 lr: 0.005\n",
      "iteration: 6620 loss: 0.0083 lr: 0.005\n",
      "iteration: 6630 loss: 0.0060 lr: 0.005\n",
      "iteration: 6640 loss: 0.0055 lr: 0.005\n",
      "iteration: 6650 loss: 0.0054 lr: 0.005\n",
      "iteration: 6660 loss: 0.0059 lr: 0.005\n",
      "iteration: 6670 loss: 0.0071 lr: 0.005\n",
      "iteration: 6680 loss: 0.0060 lr: 0.005\n",
      "iteration: 6690 loss: 0.0064 lr: 0.005\n",
      "iteration: 6700 loss: 0.0064 lr: 0.005\n",
      "iteration: 6710 loss: 0.0058 lr: 0.005\n",
      "iteration: 6720 loss: 0.0068 lr: 0.005\n",
      "iteration: 6730 loss: 0.0065 lr: 0.005\n",
      "iteration: 6740 loss: 0.0051 lr: 0.005\n",
      "iteration: 6750 loss: 0.0086 lr: 0.005\n",
      "iteration: 6760 loss: 0.0050 lr: 0.005\n",
      "iteration: 6770 loss: 0.0064 lr: 0.005\n",
      "iteration: 6780 loss: 0.0112 lr: 0.005\n",
      "iteration: 6790 loss: 0.0062 lr: 0.005\n",
      "iteration: 6800 loss: 0.0065 lr: 0.005\n",
      "iteration: 6810 loss: 0.0046 lr: 0.005\n",
      "iteration: 6820 loss: 0.0065 lr: 0.005\n",
      "iteration: 6830 loss: 0.0064 lr: 0.005\n",
      "iteration: 6840 loss: 0.0068 lr: 0.005\n",
      "iteration: 6850 loss: 0.0060 lr: 0.005\n",
      "iteration: 6860 loss: 0.0075 lr: 0.005\n",
      "iteration: 6870 loss: 0.0054 lr: 0.005\n",
      "iteration: 6880 loss: 0.0065 lr: 0.005\n",
      "iteration: 6890 loss: 0.0060 lr: 0.005\n",
      "iteration: 6900 loss: 0.0052 lr: 0.005\n",
      "iteration: 6910 loss: 0.0051 lr: 0.005\n",
      "iteration: 6920 loss: 0.0067 lr: 0.005\n",
      "iteration: 6930 loss: 0.0064 lr: 0.005\n",
      "iteration: 6940 loss: 0.0047 lr: 0.005\n",
      "iteration: 6950 loss: 0.0074 lr: 0.005\n",
      "iteration: 6960 loss: 0.0065 lr: 0.005\n",
      "iteration: 6970 loss: 0.0070 lr: 0.005\n",
      "iteration: 6980 loss: 0.0053 lr: 0.005\n",
      "iteration: 6990 loss: 0.0060 lr: 0.005\n",
      "iteration: 7000 loss: 0.0099 lr: 0.005\n",
      "iteration: 7010 loss: 0.0070 lr: 0.005\n",
      "iteration: 7020 loss: 0.0097 lr: 0.005\n",
      "iteration: 7030 loss: 0.0084 lr: 0.005\n",
      "iteration: 7040 loss: 0.0064 lr: 0.005\n",
      "iteration: 7050 loss: 0.0079 lr: 0.005\n",
      "iteration: 7060 loss: 0.0069 lr: 0.005\n",
      "iteration: 7070 loss: 0.0056 lr: 0.005\n",
      "iteration: 7080 loss: 0.0068 lr: 0.005\n",
      "iteration: 7090 loss: 0.0063 lr: 0.005\n",
      "iteration: 7100 loss: 0.0061 lr: 0.005\n",
      "iteration: 7110 loss: 0.0073 lr: 0.005\n",
      "iteration: 7120 loss: 0.0078 lr: 0.005\n",
      "iteration: 7130 loss: 0.0071 lr: 0.005\n",
      "iteration: 7140 loss: 0.0082 lr: 0.005\n",
      "iteration: 7150 loss: 0.0053 lr: 0.005\n",
      "iteration: 7160 loss: 0.0067 lr: 0.005\n",
      "iteration: 7170 loss: 0.0096 lr: 0.005\n",
      "iteration: 7180 loss: 0.0075 lr: 0.005\n",
      "iteration: 7190 loss: 0.0063 lr: 0.005\n",
      "iteration: 7200 loss: 0.0062 lr: 0.005\n",
      "iteration: 7210 loss: 0.0072 lr: 0.005\n",
      "iteration: 7220 loss: 0.0053 lr: 0.005\n",
      "iteration: 7230 loss: 0.0056 lr: 0.005\n",
      "iteration: 7240 loss: 0.0085 lr: 0.005\n",
      "iteration: 7250 loss: 0.0083 lr: 0.005\n",
      "iteration: 7260 loss: 0.0069 lr: 0.005\n",
      "iteration: 7270 loss: 0.0071 lr: 0.005\n",
      "iteration: 7280 loss: 0.0093 lr: 0.005\n",
      "iteration: 7290 loss: 0.0073 lr: 0.005\n",
      "iteration: 7300 loss: 0.0065 lr: 0.005\n",
      "iteration: 7310 loss: 0.0054 lr: 0.005\n",
      "iteration: 7320 loss: 0.0052 lr: 0.005\n",
      "iteration: 7330 loss: 0.0054 lr: 0.005\n",
      "iteration: 7340 loss: 0.0057 lr: 0.005\n",
      "iteration: 7350 loss: 0.0053 lr: 0.005\n",
      "iteration: 7360 loss: 0.0046 lr: 0.005\n",
      "iteration: 7370 loss: 0.0061 lr: 0.005\n",
      "iteration: 7380 loss: 0.0056 lr: 0.005\n",
      "iteration: 7390 loss: 0.0046 lr: 0.005\n",
      "iteration: 7400 loss: 0.0069 lr: 0.005\n",
      "iteration: 7410 loss: 0.0062 lr: 0.005\n",
      "iteration: 7420 loss: 0.0066 lr: 0.005\n",
      "iteration: 7430 loss: 0.0069 lr: 0.005\n",
      "iteration: 7440 loss: 0.0055 lr: 0.005\n",
      "iteration: 7450 loss: 0.0058 lr: 0.005\n",
      "iteration: 7460 loss: 0.0042 lr: 0.005\n",
      "iteration: 7470 loss: 0.0044 lr: 0.005\n",
      "iteration: 7480 loss: 0.0058 lr: 0.005\n",
      "iteration: 7490 loss: 0.0062 lr: 0.005\n",
      "iteration: 7500 loss: 0.0069 lr: 0.005\n",
      "iteration: 7510 loss: 0.0047 lr: 0.005\n",
      "iteration: 7520 loss: 0.0058 lr: 0.005\n",
      "iteration: 7530 loss: 0.0049 lr: 0.005\n",
      "iteration: 7540 loss: 0.0071 lr: 0.005\n",
      "iteration: 7550 loss: 0.0068 lr: 0.005\n",
      "iteration: 7560 loss: 0.0051 lr: 0.005\n",
      "iteration: 7570 loss: 0.0060 lr: 0.005\n",
      "iteration: 7580 loss: 0.0086 lr: 0.005\n",
      "iteration: 7590 loss: 0.0046 lr: 0.005\n",
      "iteration: 7600 loss: 0.0052 lr: 0.005\n",
      "iteration: 7610 loss: 0.0056 lr: 0.005\n",
      "iteration: 7620 loss: 0.0074 lr: 0.005\n",
      "iteration: 7630 loss: 0.0073 lr: 0.005\n",
      "iteration: 7640 loss: 0.0107 lr: 0.005\n",
      "iteration: 7650 loss: 0.0045 lr: 0.005\n",
      "iteration: 7660 loss: 0.0060 lr: 0.005\n",
      "iteration: 7670 loss: 0.0065 lr: 0.005\n",
      "iteration: 7680 loss: 0.0066 lr: 0.005\n",
      "iteration: 7690 loss: 0.0053 lr: 0.005\n",
      "iteration: 7700 loss: 0.0049 lr: 0.005\n",
      "iteration: 7710 loss: 0.0053 lr: 0.005\n",
      "iteration: 7720 loss: 0.0084 lr: 0.005\n",
      "iteration: 7730 loss: 0.0046 lr: 0.005\n",
      "iteration: 7740 loss: 0.0062 lr: 0.005\n",
      "iteration: 7750 loss: 0.0067 lr: 0.005\n",
      "iteration: 7760 loss: 0.0051 lr: 0.005\n",
      "iteration: 7770 loss: 0.0056 lr: 0.005\n",
      "iteration: 7780 loss: 0.0090 lr: 0.005\n",
      "iteration: 7790 loss: 0.0049 lr: 0.005\n",
      "iteration: 7800 loss: 0.0067 lr: 0.005\n",
      "iteration: 7810 loss: 0.0052 lr: 0.005\n",
      "iteration: 7820 loss: 0.0072 lr: 0.005\n",
      "iteration: 7830 loss: 0.0061 lr: 0.005\n",
      "iteration: 7840 loss: 0.0056 lr: 0.005\n",
      "iteration: 7850 loss: 0.0054 lr: 0.005\n",
      "iteration: 7860 loss: 0.0094 lr: 0.005\n",
      "iteration: 7870 loss: 0.0082 lr: 0.005\n",
      "iteration: 7880 loss: 0.0089 lr: 0.005\n",
      "iteration: 7890 loss: 0.0066 lr: 0.005\n",
      "iteration: 7900 loss: 0.0062 lr: 0.005\n",
      "iteration: 7910 loss: 0.0064 lr: 0.005\n",
      "iteration: 7920 loss: 0.0063 lr: 0.005\n",
      "iteration: 7930 loss: 0.0049 lr: 0.005\n",
      "iteration: 7940 loss: 0.0097 lr: 0.005\n",
      "iteration: 7950 loss: 0.0065 lr: 0.005\n",
      "iteration: 7960 loss: 0.0054 lr: 0.005\n",
      "iteration: 7970 loss: 0.0076 lr: 0.005\n",
      "iteration: 7980 loss: 0.0061 lr: 0.005\n",
      "iteration: 7990 loss: 0.0063 lr: 0.005\n",
      "iteration: 8000 loss: 0.0094 lr: 0.005\n",
      "iteration: 8010 loss: 0.0053 lr: 0.005\n",
      "iteration: 8020 loss: 0.0072 lr: 0.005\n",
      "iteration: 8030 loss: 0.0068 lr: 0.005\n",
      "iteration: 8040 loss: 0.0076 lr: 0.005\n",
      "iteration: 8050 loss: 0.0056 lr: 0.005\n",
      "iteration: 8060 loss: 0.0084 lr: 0.005\n",
      "iteration: 8070 loss: 0.0074 lr: 0.005\n",
      "iteration: 8080 loss: 0.0058 lr: 0.005\n",
      "iteration: 8090 loss: 0.0084 lr: 0.005\n",
      "iteration: 8100 loss: 0.0059 lr: 0.005\n",
      "iteration: 8110 loss: 0.0057 lr: 0.005\n",
      "iteration: 8120 loss: 0.0071 lr: 0.005\n",
      "iteration: 8130 loss: 0.0054 lr: 0.005\n",
      "iteration: 8140 loss: 0.0068 lr: 0.005\n",
      "iteration: 8150 loss: 0.0058 lr: 0.005\n",
      "iteration: 8160 loss: 0.0090 lr: 0.005\n",
      "iteration: 8170 loss: 0.0055 lr: 0.005\n",
      "iteration: 8180 loss: 0.0055 lr: 0.005\n",
      "iteration: 8190 loss: 0.0065 lr: 0.005\n",
      "iteration: 8200 loss: 0.0065 lr: 0.005\n",
      "iteration: 8210 loss: 0.0058 lr: 0.005\n",
      "iteration: 8220 loss: 0.0054 lr: 0.005\n",
      "iteration: 8230 loss: 0.0059 lr: 0.005\n",
      "iteration: 8240 loss: 0.0079 lr: 0.005\n",
      "iteration: 8250 loss: 0.0057 lr: 0.005\n",
      "iteration: 8260 loss: 0.0082 lr: 0.005\n",
      "iteration: 8270 loss: 0.0061 lr: 0.005\n",
      "iteration: 8280 loss: 0.0044 lr: 0.005\n",
      "iteration: 8290 loss: 0.0076 lr: 0.005\n",
      "iteration: 8300 loss: 0.0049 lr: 0.005\n",
      "iteration: 8310 loss: 0.0058 lr: 0.005\n",
      "iteration: 8320 loss: 0.0059 lr: 0.005\n",
      "iteration: 8330 loss: 0.0070 lr: 0.005\n",
      "iteration: 8340 loss: 0.0067 lr: 0.005\n",
      "iteration: 8350 loss: 0.0052 lr: 0.005\n",
      "iteration: 8360 loss: 0.0049 lr: 0.005\n",
      "iteration: 8370 loss: 0.0066 lr: 0.005\n",
      "iteration: 8380 loss: 0.0060 lr: 0.005\n",
      "iteration: 8390 loss: 0.0079 lr: 0.005\n",
      "iteration: 8400 loss: 0.0057 lr: 0.005\n",
      "iteration: 8410 loss: 0.0051 lr: 0.005\n",
      "iteration: 8420 loss: 0.0051 lr: 0.005\n",
      "iteration: 8430 loss: 0.0048 lr: 0.005\n",
      "iteration: 8440 loss: 0.0063 lr: 0.005\n",
      "iteration: 8450 loss: 0.0054 lr: 0.005\n",
      "iteration: 8460 loss: 0.0097 lr: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 8470 loss: 0.0055 lr: 0.005\n",
      "iteration: 8480 loss: 0.0061 lr: 0.005\n",
      "iteration: 8490 loss: 0.0057 lr: 0.005\n",
      "iteration: 8500 loss: 0.0058 lr: 0.005\n",
      "iteration: 8510 loss: 0.0064 lr: 0.005\n",
      "iteration: 8520 loss: 0.0049 lr: 0.005\n",
      "iteration: 8530 loss: 0.0052 lr: 0.005\n",
      "iteration: 8540 loss: 0.0055 lr: 0.005\n",
      "iteration: 8550 loss: 0.0076 lr: 0.005\n",
      "iteration: 8560 loss: 0.0051 lr: 0.005\n",
      "iteration: 8570 loss: 0.0055 lr: 0.005\n",
      "iteration: 8580 loss: 0.0056 lr: 0.005\n",
      "iteration: 8590 loss: 0.0049 lr: 0.005\n",
      "iteration: 8600 loss: 0.0055 lr: 0.005\n",
      "iteration: 8610 loss: 0.0059 lr: 0.005\n",
      "iteration: 8620 loss: 0.0055 lr: 0.005\n",
      "iteration: 8630 loss: 0.0071 lr: 0.005\n",
      "iteration: 8640 loss: 0.0071 lr: 0.005\n",
      "iteration: 8650 loss: 0.0056 lr: 0.005\n",
      "iteration: 8660 loss: 0.0044 lr: 0.005\n",
      "iteration: 8670 loss: 0.0044 lr: 0.005\n",
      "iteration: 8680 loss: 0.0048 lr: 0.005\n",
      "iteration: 8690 loss: 0.0043 lr: 0.005\n",
      "iteration: 8700 loss: 0.0073 lr: 0.005\n",
      "iteration: 8710 loss: 0.0065 lr: 0.005\n",
      "iteration: 8720 loss: 0.0051 lr: 0.005\n",
      "iteration: 8730 loss: 0.0068 lr: 0.005\n",
      "iteration: 8740 loss: 0.0050 lr: 0.005\n",
      "iteration: 8750 loss: 0.0054 lr: 0.005\n",
      "iteration: 8760 loss: 0.0046 lr: 0.005\n",
      "iteration: 8770 loss: 0.0041 lr: 0.005\n",
      "iteration: 8780 loss: 0.0057 lr: 0.005\n",
      "iteration: 8790 loss: 0.0059 lr: 0.005\n",
      "iteration: 8800 loss: 0.0040 lr: 0.005\n",
      "iteration: 8810 loss: 0.0050 lr: 0.005\n",
      "iteration: 8820 loss: 0.0045 lr: 0.005\n",
      "iteration: 8830 loss: 0.0053 lr: 0.005\n",
      "iteration: 8840 loss: 0.0062 lr: 0.005\n",
      "iteration: 8850 loss: 0.0095 lr: 0.005\n",
      "iteration: 8860 loss: 0.0063 lr: 0.005\n",
      "iteration: 8870 loss: 0.0090 lr: 0.005\n",
      "iteration: 8880 loss: 0.0047 lr: 0.005\n",
      "iteration: 8890 loss: 0.0044 lr: 0.005\n",
      "iteration: 8900 loss: 0.0041 lr: 0.005\n",
      "iteration: 8910 loss: 0.0052 lr: 0.005\n",
      "iteration: 8920 loss: 0.0051 lr: 0.005\n",
      "iteration: 8930 loss: 0.0051 lr: 0.005\n",
      "iteration: 8940 loss: 0.0050 lr: 0.005\n",
      "iteration: 8950 loss: 0.0073 lr: 0.005\n",
      "iteration: 8960 loss: 0.0056 lr: 0.005\n",
      "iteration: 8970 loss: 0.0075 lr: 0.005\n",
      "iteration: 8980 loss: 0.0062 lr: 0.005\n",
      "iteration: 8990 loss: 0.0061 lr: 0.005\n",
      "iteration: 9000 loss: 0.0047 lr: 0.005\n",
      "iteration: 9010 loss: 0.0042 lr: 0.005\n",
      "iteration: 9020 loss: 0.0060 lr: 0.005\n",
      "iteration: 9030 loss: 0.0049 lr: 0.005\n",
      "iteration: 9040 loss: 0.0042 lr: 0.005\n",
      "iteration: 9050 loss: 0.0079 lr: 0.005\n",
      "iteration: 9060 loss: 0.0047 lr: 0.005\n",
      "iteration: 9070 loss: 0.0099 lr: 0.005\n",
      "iteration: 9080 loss: 0.0053 lr: 0.005\n",
      "iteration: 9090 loss: 0.0048 lr: 0.005\n",
      "iteration: 9100 loss: 0.0057 lr: 0.005\n",
      "iteration: 9110 loss: 0.0085 lr: 0.005\n",
      "iteration: 9120 loss: 0.0050 lr: 0.005\n",
      "iteration: 9130 loss: 0.0080 lr: 0.005\n",
      "iteration: 9140 loss: 0.0065 lr: 0.005\n",
      "iteration: 9150 loss: 0.0057 lr: 0.005\n",
      "iteration: 9160 loss: 0.0058 lr: 0.005\n",
      "iteration: 9170 loss: 0.0056 lr: 0.005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5814fc64e079>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplayiters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaveiters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\DLC-GPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DLC-GPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(config, shuffle, trainingsetindex, max_snapshots_to_keep, displayiters, saveiters, maxiters, allow_growth, gputouse, autotune, keepdeconvweights, modelprefix)\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mmax_to_keep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_snapshots_to_keep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mkeepdeconvweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdeconvweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                 \u001b[0mallow_growth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m             )  # pass on path and file name for pose_cfg.yaml!\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DLC-GPU\\lib\\site-packages\\deeplabcut\\pose_estimation_tensorflow\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(config_yaml, displayiters, saveiters, maxiters, max_to_keep, keepdeconvweights, allow_growth)\u001b[0m\n\u001b[0;32m    276\u001b[0m         [_, loss_val, summary] = sess.run(\n\u001b[0;32m    277\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged_summaries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m         )\n\u001b[0;32m    280\u001b[0m         \u001b[0mcum_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DLC-GPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DLC-GPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DLC-GPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DLC-GPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DLC-GPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DLC-GPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10, saveiters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note, that if it reaches the end or you stop it (by hitting \"stop\" or by CTRL+C), \n",
    "you will see an \"KeyboardInterrupt\" error, but you can ignore this!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCzxVT_gywnc"
   },
   "source": [
    "## Evaluate a trained network\n",
    "\n",
    "This function evaluates a trained model for a specific shuffle/shuffles at a particular training state (snapshot) or on all the states. The network is evaluated on the data set (images) and stores the results as .csv file in a subdirectory under **evaluation-results**.\n",
    "\n",
    "You can change various parameters in the ```config.yaml``` file of this project. For evaluation all the model descriptors (Task, TrainingFraction, Date etc.) are important. For the evaluation one can change pcutoff. This cutoff also influences how likely estimated postions need to be so that they are shown in the plots. One can furthermore, change the colormap and dotsize for those graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuprPKDdywne",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  DLC_resnet_50_openfieldOct30shuffle1_9100  with # of trainingiterations: 9100\n",
      "Initializing ResNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [00:04, 23.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done and results stored for snapshot:  snapshot-9100\n",
      "Results for 9100  training iterations: 95 1 train error: 2.53 pixels. Test error: 2.41  pixels.\n",
      "With pcutoff of 0.4  train error: 2.53 pixels. Test error: 2.41 pixels\n",
      "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
      "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
      "If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\n",
      "Use the function 'analyze_video' to make predictions on new videos.\n",
      "Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\n"
     ]
    }
   ],
   "source": [
    "deeplabcut.evaluate_network(path_config_file,plotting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: depending on your set up sometimes you get some \"matplotlib errors, but these are not important*\n",
    "\n",
    "Now you can go check out the images. Given the limted data input and it took ~20 mins to test this out, it is not meant to track well, so don't be alarmed. This is just to get you familiar with the workflow... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeqYWGaXywnj"
   },
   "source": [
    "## Analyzing videos\n",
    "This function extracts the pose based on a trained network from videos. The user can choose the trained network - by default the most recent snapshot is used to analyse the videos. However, the user can also specify the snapshot index for the variable **snapshotindex** in the **config.yaml** file).\n",
    "\n",
    "The results are stored in hd5 file in the same directory, where the video resides. The pose array (pose vs. frame index) can also be exported as csv file (set flag to...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vv9iHHLlywnl"
   },
   "outputs": [],
   "source": [
    "# Creating video path:\n",
    "import os\n",
    "videofile_path = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30/videos/m3v1mp4.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFbPPD4hywnq",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start analyzing the video!\n",
      "Using snapshot-9100 for model C:\\Users\\Public\\GitHub\\DeepLabCut\\examples\\openfield-Pranav-2018-10-30\\dlc-models\\iteration-0\\openfieldOct30-trainset95shuffle1\n",
      "Initializing ResNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/2330 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  C:\\Users\\Public\\GitHub\\DeepLabCut\\examples\\openfield-Pranav-2018-10-30/videos/m3v1mp4.mp4\n",
      "C:\\Users\\Public\\GitHub\\DeepLabCut\\examples\\openfield-Pranav-2018-10-30\\videos  already exists!\n",
      "Loading  C:\\Users\\Public\\GitHub\\DeepLabCut\\examples\\openfield-Pranav-2018-10-30/videos/m3v1mp4.mp4\n",
      "Duration of video [s]:  77.67 , recorded with  30.0 fps!\n",
      "Overall # of frames:  2330  found with (before cropping) frame dimensions:  640 480\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2346it [00:36, 63.43it/s]                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in C:\\Users\\Public\\GitHub\\DeepLabCut\\examples\\openfield-Pranav-2018-10-30\\videos...\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DLC_resnet_50_openfieldOct30shuffle1_9100'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Start analyzing the video!\")\n",
    "#our demo video on a CPU with take ~30 min to analze! GPU is much faster!\n",
    "deeplabcut.analyze_videos(path_config_file,[videofile_path], save_as_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQ3T3oykywnw"
   },
   "source": [
    "## Create labeled video\n",
    "\n",
    "This function is for the visualization purpose and can be used to create a video in .mp4 format with the predicted labels. This video is saved in the same directory, where the (unlabeled) video resides. \n",
    "\n",
    "Various parameters can be set with regard to the colormap and the dotsize. The parameters of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhI9KLs4ywn0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,[videofile_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IErvm1K5ywn5"
   },
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color. The underlying functions can easily be customized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mP2useJgywn7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "deeplabcut.plot_trajectories(path_config_file,[videofile_path],showfigures=True)\n",
    "\n",
    "#These plots can are interactive and can be customized (see https://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5nOaWzXywoB"
   },
   "source": [
    "## Extract outlier frames, where the predictions are off.\n",
    "\n",
    "This is optional step allows to add more training data when the evaluation results are poor. In such a case, the user can use the following function to extract frames where the labels are incorrectly predicted. Make sure to provide the correct value of the \"iterations\" as it will be used to create the unique directory where the extracted frames will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJGiDKuUywoC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabcut.extract_outlier_frames(path_config_file,[videofile_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHjpscPcywoG"
   },
   "source": [
    "The user can run this iteratively, and (even) extract additional frames from the same video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uaNUm3NSywoH"
   },
   "source": [
    "## Manually correct labels\n",
    "\n",
    "This step allows the user to correct the labels in the extracted frames. Navigate to the folder corresponding to the video 'm3v1mp4' and use the GUI as described in the protocol to update the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJDvJMcrywoI"
   },
   "outputs": [],
   "source": [
    "%gui wx\n",
    "deeplabcut.refine_labels(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7efellnywoT"
   },
   "outputs": [],
   "source": [
    "#Perhaps plot the labels to see how how all the frames are annoted (including the refined ones)\n",
    "deeplabcut.check_labels(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcuqoeRbywoL"
   },
   "outputs": [],
   "source": [
    "# Now merge datasets (once you refined all frames)\n",
    "deeplabcut.merge_datasets(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRB9KgGsywoP"
   },
   "source": [
    "## Create a new iteration of training dataset, check it and train...\n",
    "\n",
    "Following the refine labels, append these frames to the original dataset to create a new iteration of training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGHghXfdywoQ"
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fhL6nG2ywoW"
   },
   "source": [
    "Now one can train the network again... (with the expanded data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAUxg5sgywoX"
   },
   "outputs": [],
   "source": [
    "deeplabcut.train_network(path_config_file, shuffle=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Demo-labeledexample-MouseReaching.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-DLC-GPU] *",
   "language": "python",
   "name": "conda-env-.conda-DLC-GPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
